{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "6C_TzBZvgtx7",
        "outputId": "1ed7e1f0-089f-4948-a50d-b5a874adcec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "housing_discrete.csv  spark-3.1.1-bin-hadoop3.2\n",
            "outcome_bins.csv      spark-3.1.1-bin-hadoop3.2.tgz\n",
            "sample_data\t      spark-3.1.1-bin-hadoop3.2.tgz.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f379576c1c0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e355f8937880:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "!ls\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dataset (discrete dataset for variables as well as outcomes - preprocessing done in jupyter)\n",
        "from pyspark.sql.functions import monotonically_increasing_id \n",
        "import pandas as pd\n",
        "\n",
        "discrete_df = spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \"\\t\").load(\"housing_discrete.csv\")\n",
        "discrete_df = discrete_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "discrete_df.show()\n",
        "filtered_df = discrete_df.filter((col(\"indus\") == 0) & (col(\"dis\") == 2))\n",
        "filtered_df.show()\n",
        "\n",
        "\n",
        "# print(discrete_df.count()) # avoid count in final code as it is an \"action\" in spark\n",
        "outcomes_df = spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \"\\t\").load(\"outcome_bins.csv\")\n",
        "outcomes_df = outcomes_df.toDF(\"medv_bin\")\n",
        "outcomes_df = outcomes_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "# print(outcomes_df.count()) # avoid count in final code as it is an \"action\" in spark\n",
        "\n",
        "df = discrete_df.join(outcomes_df, \"id\")\n",
        "df.drop(\"id\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj5T72RYvqfO",
        "outputId": "777c9f78-c6a9-4ea7-9839-93abaf3f73a0"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+---+\n",
            "|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat| id|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+---+\n",
            "|   0|  1|    0|   0|  2|  2|  1|  2|  0|  1|      0|    3|    0|  0|\n",
            "|   0|  0|    1|   0|  1|  2|  2|  2|  0|  0|      1|    3|    1|  1|\n",
            "|   0|  0|    1|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|  2|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    2|    0|  3|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    3|    0|  4|\n",
            "|   0|  0|    0|   0|  1|  2|  1|  3|  0|  0|      1|    2|    0|  5|\n",
            "|   0|  1|    1|   0|  1|  1|  1|  3|  2|  1|      0|    2|    2|  6|\n",
            "|   0|  1|    1|   0|  1|  1|  3|  3|  2|  1|      0|    3|    3|  7|\n",
            "|   0|  1|    1|   0|  1|  0|  3|  3|  2|  1|      0|    1|    3|  8|\n",
            "|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    1|    2|  9|\n",
            "|   0|  1|    1|   0|  1|  2|  3|  3|  2|  1|      0|    2|    3| 10|\n",
            "|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    3|    2| 11|\n",
            "|   0|  1|    1|   0|  1|  1|  0|  3|  2|  1|      0|    1|    2| 12|\n",
            "|   0|  0|    1|   0|  2|  1|  1|  2|  1|  1|      3|    3|    1| 13|\n",
            "|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    1| 14|\n",
            "|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    2|    1| 15|\n",
            "|   0|  0|    1|   0|  2|  1|  0|  2|  1|  1|      3|    1|    0| 16|\n",
            "|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    2| 17|\n",
            "|   0|  0|    1|   0|  2|  0|  0|  2|  1|  1|      3|    0|    2| 18|\n",
            "|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    1|    1| 19|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+---+\n",
            "|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat| id|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+---+\n",
            "|   0|  1|    0|   0|  2|  2|  1|  2|  0|  1|      0|    3|    0|  0|\n",
            "|   0|  0|    0|   0|  1|  2|  1|  2|  0|  0|      1|    3|    1|  1|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|  2|\n",
            "|   0|  0|    0|   0|  1|  1|  1|  2|  0|  0|      1|    2|    2|  3|\n",
            "|   0|  0|    0|   0|  1|  1|  1|  2|  0|  0|      1|    2|    1|  4|\n",
            "|   0|  0|    0|   0|  1|  3|  2|  2|  0|  0|      1|    3|    0|  5|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|  6|\n",
            "|   0|  0|    0|   0|  0|  3|  1|  2|  0|  0|      1|    0|    0|  7|\n",
            "|   0|  0|    0|   0|  0|  1|  1|  2|  0|  0|      1|    2|    1|  8|\n",
            "|   0|  0|    0|   0|  0|  3|  1|  2|  0|  0|      1|    3|    0|  9|\n",
            "|   0|  0|    0|   0|  0|  3|  0|  2|  0|  0|      1|    2|    0| 10|\n",
            "|   0|  0|    0|   0|  0|  3|  1|  2|  0|  0|      1|    3|    0| 11|\n",
            "|   0|  0|    0|   0|  1|  1|  1|  2|  2|  1|      0|    2|    1| 12|\n",
            "|   0|  0|    0|   0|  1|  2|  1|  2|  2|  1|      0|    2|    0| 13|\n",
            "|   0|  0|    0|   0|  1|  1|  1|  2|  0|  0|      1|    1|    2| 14|\n",
            "|   0|  1|    0|   0|  0|  3|  0|  2|  2|  2|      0|    2|    0| 15|\n",
            "|   0|  1|    0|   0|  0|  2|  0|  2|  2|  2|      0|    1|    0| 16|\n",
            "|   0|  1|    0|   0|  0|  3|  0|  2|  2|  2|      0|    3|    0| 17|\n",
            "|   0|  1|    0|   0|  0|  3|  0|  2|  1|  0|      0|    2|    0| 18|\n",
            "|   0|  1|    0|   0|  0|  3|  0|  2|  2|  0|      0|    3|    0| 19|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+--------+\n",
            "| id|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat|medv_bin|\n",
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+--------+\n",
            "|  0|   0|  1|    0|   0|  2|  2|  1|  2|  0|  1|      0|    3|    0|       5|\n",
            "|  1|   0|  0|    1|   0|  1|  2|  2|  2|  0|  0|      1|    3|    1|       4|\n",
            "|  2|   0|  0|    1|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|       7|\n",
            "|  3|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    2|    0|       7|\n",
            "|  4|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    3|    0|       8|\n",
            "|  5|   0|  0|    0|   0|  1|  2|  1|  3|  0|  0|      1|    2|    0|       6|\n",
            "|  6|   0|  1|    1|   0|  1|  1|  1|  3|  2|  1|      0|    2|    2|       5|\n",
            "|  7|   0|  1|    1|   0|  1|  1|  3|  3|  2|  1|      0|    3|    3|       6|\n",
            "|  8|   0|  1|    1|   0|  1|  0|  3|  3|  2|  1|      0|    1|    3|       3|\n",
            "|  9|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    1|    2|       4|\n",
            "| 10|   0|  1|    1|   0|  1|  2|  3|  3|  2|  1|      0|    2|    3|       3|\n",
            "| 11|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    3|    2|       4|\n",
            "| 12|   0|  1|    1|   0|  1|  1|  0|  3|  2|  1|      0|    1|    2|       4|\n",
            "| 13|   0|  0|    1|   0|  2|  1|  1|  2|  1|  1|      3|    3|    1|       4|\n",
            "| 14|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    1|       4|\n",
            "| 15|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    2|    1|       4|\n",
            "| 16|   0|  0|    1|   0|  2|  1|  0|  2|  1|  1|      3|    1|    0|       5|\n",
            "| 17|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    2|       3|\n",
            "| 18|   0|  0|    1|   0|  2|  0|  0|  2|  1|  1|      3|    0|    2|       4|\n",
            "| 19|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    1|    1|       4|\n",
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate entropy, mean, stddev for outcomes (all rows) which is common irrespective of what slice we pick\n",
        "\n",
        "from pyspark.sql.functions import col, count, sum, col, log2, mean, stddev\n",
        "\n",
        "outcomes = df.select(\"medv_bin\")\n",
        "outcomes.show()\n",
        "grouped_data = outcomes_df.groupBy(\"medv_bin\").count()\n",
        "grouped_data = grouped_data.withColumn(\"count\", col(\"count\").cast(\"int\"))\n",
        "grouped_data.show()\n",
        "total_count = grouped_data.agg(sum(\"count\")).collect()[0][0]\n",
        "\n",
        "probabilities = grouped_data.withColumn(\"probability\", (-1 * (col(\"count\") / total_count) * log2(col(\"count\") / total_count)))\n",
        "# probabilities.show()\n",
        "\n",
        "outcome_entropy = probabilities.agg(sum(\"probability\")).collect()[0][0]\n",
        "outcome_mean = outcomes.select(mean(\"medv_bin\")).collect()[0][0]\n",
        "outcome_stddev = df.select(stddev(\"medv_bin\")).collect()[0][0]\n",
        "\n",
        "print (\"outcome_entropy: \", outcome_entropy)\n",
        "print (\"outcome_mean: \", outcome_mean)\n",
        "print (\"outcome_stddev: \", outcome_stddev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dqOe1Jivi5E",
        "outputId": "e949349b-1c5d-46cf-a1fa-b88340adefd9"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|medv_bin|\n",
            "+--------+\n",
            "|       5|\n",
            "|       4|\n",
            "|       7|\n",
            "|       7|\n",
            "|       8|\n",
            "|       6|\n",
            "|       5|\n",
            "|       6|\n",
            "|       3|\n",
            "|       4|\n",
            "|       3|\n",
            "|       4|\n",
            "|       4|\n",
            "|       4|\n",
            "|       4|\n",
            "|       4|\n",
            "|       5|\n",
            "|       3|\n",
            "|       4|\n",
            "|       4|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------+-----+\n",
            "|medv_bin|count|\n",
            "+--------+-----+\n",
            "|       7|   30|\n",
            "|      11|    1|\n",
            "|       3|   81|\n",
            "|       8|   13|\n",
            "|       5|   96|\n",
            "|       6|   39|\n",
            "|       9|    8|\n",
            "|       1|   21|\n",
            "|      10|    6|\n",
            "|       4|  147|\n",
            "|       2|   48|\n",
            "+--------+-----+\n",
            "\n",
            "outcome_entropy:  2.8033607009396215\n",
            "outcome_mean:  4.3244897959183675\n",
            "outcome_stddev:  1.811915865931041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a row from the dataframe\n",
        "sample_row = df.sample(0.1, seed=42).limit(1)\n",
        "print (\"sample_row:\\n\\n\", sample_row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHw51RvmnuT0",
        "outputId": "c2f9b5f2-41ee-43a8-a327-a0499ce28bd9"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_row:\n",
            "\n",
            " +---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+--------+\n",
            "| id|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat|medv_bin|\n",
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+--------+\n",
            "|  7|   0|  1|    1|   0|  1|  1|  3|  3|  2|  1|      0|    3|    3|       6|\n",
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "import heapq\n",
        "\n",
        "# Initialize ranked_slices dictionary\n",
        "ranked_slices = {}\n",
        "\n",
        "# Initialize score_cache dictionary\n",
        "score_cache = {}\n",
        "\n",
        "# top k slices\n",
        "k = 10\n",
        "\n",
        "# max number of features to consider for slice\n",
        "M = 5\n",
        "\n",
        "feature_columns = discrete_df.columns\n",
        "\n",
        "# to consider max nuber of features, uncomment following:\n",
        "# M = len(feature_columns)\n",
        "\n",
        "# print (feature_columns)\n",
        "for m in range(1, M + 1):\n",
        "  if (m == 1):\n",
        "    for column in feature_columns:\n",
        "      if (column == 'id'):\n",
        "        continue\n",
        "      # print (column)\n",
        "      column_value = sample_row.first()[column]\n",
        "      # print(column_value)\n",
        "      example_slice_df = df.filter(col(column) == column_value)\n",
        "      slice_outcomes = example_slice_df.select(\"medv_bin\")\n",
        "      slice_grouped_data = slice_outcomes.groupBy(\"medv_bin\").count()\n",
        "      slice_grouped_data = slice_grouped_data.withColumn(\"count\", col(\"count\").cast(\"int\"))\n",
        "      slice_count = slice_grouped_data.agg(sum(\"count\")).collect()[0][0]\n",
        "\n",
        "      slice_probabilities = slice_grouped_data.withColumn(\"probability\", (-1 * (col(\"count\") / slice_count) * log2(col(\"count\") / slice_count)))\n",
        "\n",
        "      slice_outcome_entropy = slice_probabilities.agg(sum(\"probability\")).collect()[0][0]\n",
        "      # slice_outcome_mean = slice_outcomes.select(mean(\"medv_bin\")).collect()[0][0]\n",
        "\n",
        "      # Check if the current slice's entropy score exceeds any existing entry\n",
        "      exceed_existing = all(slice_outcome_entropy >= score for score in ranked_slices.values())\n",
        "      # Add the slice to ranked_slices only if it exceeds any existing entry\n",
        "      if exceed_existing or len(ranked_slices) < k:\n",
        "        feature_vals = {}\n",
        "        feature_vals[column] = column_value\n",
        "        ranked_slices[frozenset(feature_vals.items())] = slice_outcome_entropy\n",
        "        # Limit the ranked_slices dictionary to top k entries based on entropy score\n",
        "        ranked_slices = dict(heapq.nlargest(k, ranked_slices.items(), key=lambda item: item[1]))\n",
        "      score_cache[frozenset(feature_vals.items())] = slice_outcome_entropy\n",
        "  else:\n",
        "    # print (\"m is \", m)\n",
        "    new_ranked_slices = {}\n",
        "    for base_slice in ranked_slices:\n",
        "      for column in feature_columns:\n",
        "        if column == 'id' or column in dict(base_slice):\n",
        "          continue\n",
        "        # create new slice for current feature and base slice\n",
        "        feature_vals = dict(base_slice)\n",
        "        column_value = sample_row.first()[column]\n",
        "        feature_vals[column] = column_value\n",
        "\n",
        "        # skip if the new slice already exists in ranked_slices\n",
        "        if frozenset(feature_vals.items()) in score_cache:\n",
        "          continue\n",
        "\n",
        "        print (\"running algo for slice: \", feature_vals.items())\n",
        "\n",
        "        example_slice_df = df\n",
        "        for feature, value in feature_vals.items():\n",
        "            example_slice_df = example_slice_df.filter(col(feature) == value)\n",
        "\n",
        "        slice_outcomes = example_slice_df.select(\"medv_bin\")\n",
        "        slice_grouped_data = slice_outcomes.groupBy(\"medv_bin\").count()\n",
        "        slice_grouped_data = slice_grouped_data.withColumn(\"count\", col(\"count\").cast(\"int\"))\n",
        "        slice_count = slice_grouped_data.agg(sum(\"count\")).collect()[0][0]\n",
        "        slice_probabilities = slice_grouped_data.withColumn(\"probability\", (-1 * (col(\"count\") / slice_count) * log2(col(\"count\") / slice_count)))\n",
        "        slice_outcome_entropy = slice_probabilities.agg(sum(\"probability\")).collect()[0][0]\n",
        "        new_ranked_slices[frozenset(feature_vals.items())] = slice_outcome_entropy\n",
        "        score_cache[frozenset(feature_vals.items())] = slice_outcome_entropy\n",
        "\n",
        "    ranked_slices.update(new_ranked_slices)\n",
        "    ranked_slices = dict(heapq.nlargest(k, ranked_slices.items(), key=lambda item: item[1]))"
      ],
      "metadata": {
        "id": "dlPu-qROoNAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bea629-0867-4602-a199-e40bf02ddc14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running algo for slice:  dict_items([('chas', '0'), ('crim', '0')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('zn', '1')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('indus', '1')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('nox', '1')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('rm', '1')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('age', '3')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('dis', '3')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('rad', '2')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('tax', '1')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('ptratio', '0')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('black', '3')])\n",
            "running algo for slice:  dict_items([('chas', '0'), ('lstat', '3')])\n",
            "running algo for slice:  dict_items([('crim', '0'), ('zn', '1')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Zi9zm6tKERCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "sorted_ranked_slices = dict(sorted(ranked_slices.items(), key=lambda item: item[1], reverse=True))\n",
        "pprint.pprint(sorted_ranked_slices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByeRWHRbPiB3",
        "outputId": "b2d6885e-e401-4c19-96c8-15a8f01135c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{frozenset({('chas', '0')}): 2.831358701689329,\n",
            " frozenset({('crim', '0')}): 2.6368228152510924,\n",
            " frozenset({('chas', '0'), ('crim', '0')}): 2.6161707946803947,\n",
            " frozenset({('tax', '1'), ('age', '3')}): 2.550340709546388,\n",
            " frozenset({('tax', '1'), ('crim', '0'), ('age', '3')}): 2.550340709546388,\n",
            " frozenset({('tax', '1'), ('indus', '1'), ('age', '3')}): 2.550340709546388,\n",
            " frozenset({('chas', '0'), ('tax', '1'), ('age', '3')}): 2.550340709546388,\n",
            " frozenset({('tax', '1'), ('rm', '1'), ('ptratio', '0')}): 2.550340709546388,\n",
            " frozenset({('tax', '1'), ('indus', '1'), ('crim', '0'), ('age', '3')}): 2.550340709546388,\n",
            " frozenset({('chas', '0'), ('tax', '1'), ('crim', '0'), ('age', '3')}): 2.550340709546388}\n"
          ]
        }
      ]
    }
  ]
}