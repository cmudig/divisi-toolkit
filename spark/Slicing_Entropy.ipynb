{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnLDXPQpRGL8"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "g6mP_TalRKGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDy__CSZRL5-",
        "outputId": "35606441-76a8-44d4-b3c2-fb369d23d18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "WK0K4jt6ROP0",
        "outputId": "ad13668b-581f-45b3-b8cd-877349bd0272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fa3d4255b20>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5623a7c6178d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"csv\").option(\"delimiter\", \"  \").load(\"housing.csv\")\n",
        "df.count()\n",
        "from pyspark.sql.functions import split\n",
        "df = df.select(df._c0.alias(\"crim\"),\n",
        "               df._c1.alias(\"zn\"),\n",
        "               df._c2.alias(\"indus\"),\n",
        "               df._c3.alias(\"chas\"),\n",
        "               df._c4.alias(\"nox\"),\n",
        "               df._c5.alias(\"rm\"),\n",
        "               df._c6.alias(\"age\"),\n",
        "               df._c7.alias(\"dis\"),\n",
        "               df._c8.alias(\"rad\"),\n",
        "               df._c9.alias(\"tax\"),\n",
        "               split(df._c10, \" \").getItem(0).alias(\"ptratio\"),\n",
        "               split(df._c10, \" \").getItem(1).alias(\"black\"),\n",
        "               df._c11.alias(\"lstat\"),\n",
        "               df._c12.alias(\"medv\"))\n",
        "print(df.count()) # 506\n",
        "\n",
        "df = df[df[\"medv\"] < 50]\n",
        "# print(df.count()) # 450\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkXVL6m-ROyw",
        "outputId": "98e69909-1695-4acc-cb1c-a0b66a7fe5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "506\n",
            "+--------+-----+------+----+------+------+-----+------+---+-----+-------+------+-----+-----+\n",
            "|    crim|   zn| indus|chas|   nox|    rm|  age|   dis|rad|  tax|ptratio| black|lstat| medv|\n",
            "+--------+-----+------+----+------+------+-----+------+---+-----+-------+------+-----+-----+\n",
            "| 0.00632|18.00| 2.310|   0|0.5380|6.5750|65.20|4.0900|  1|296.0|  15.30|396.90| 4.98|24.00|\n",
            "| 0.02731| 0.00| 7.070|   0|0.4690|6.4210|78.90|4.9671|  2|242.0|  17.80|396.90| 9.14|21.60|\n",
            "| 0.02729| 0.00| 7.070|   0|0.4690|7.1850|61.10|4.9671|  2|242.0|  17.80|392.83| 4.03|34.70|\n",
            "| 0.03237| 0.00| 2.180|   0|0.4580|6.9980|45.80|6.0622|  3|222.0|  18.70|394.63| 2.94|33.40|\n",
            "| 0.06905| 0.00| 2.180|   0|0.4580|7.1470|54.20|6.0622|  3|222.0|  18.70|396.90| 5.33|36.20|\n",
            "+--------+-----+------+----+------+------+-----+------+---+-----+-------+------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Check the data types of the columns\n",
        "df.printSchema()\n",
        "\n",
        "# Cast the medv column from string to double type\n",
        "df = df.withColumn(\"medv\", col(\"medv\").cast(\"double\"))\n",
        "\n",
        "# Check the data types of the columns again\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ty5LaVERSK0",
        "outputId": "6f7981ed-1fb0-4c22-f5e8-93587ee3cb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- crim: string (nullable = true)\n",
            " |-- zn: string (nullable = true)\n",
            " |-- indus: string (nullable = true)\n",
            " |-- chas: string (nullable = true)\n",
            " |-- nox: string (nullable = true)\n",
            " |-- rm: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- dis: string (nullable = true)\n",
            " |-- rad: string (nullable = true)\n",
            " |-- tax: string (nullable = true)\n",
            " |-- ptratio: string (nullable = true)\n",
            " |-- black: string (nullable = true)\n",
            " |-- lstat: string (nullable = true)\n",
            " |-- medv: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- crim: string (nullable = true)\n",
            " |-- zn: string (nullable = true)\n",
            " |-- indus: string (nullable = true)\n",
            " |-- chas: string (nullable = true)\n",
            " |-- nox: string (nullable = true)\n",
            " |-- rm: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- dis: string (nullable = true)\n",
            " |-- rad: string (nullable = true)\n",
            " |-- tax: string (nullable = true)\n",
            " |-- ptratio: string (nullable = true)\n",
            " |-- black: string (nullable = true)\n",
            " |-- lstat: string (nullable = true)\n",
            " |-- medv: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "from pyspark.sql.functions import col\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of bins\n",
        "num_bins = 10\n",
        "\n",
        "# Compute the histogram bin edges using np.histogram\n",
        "hist, outcome_bins = np.histogram(df.select(\"medv\").rdd.flatMap(lambda x: x).collect(), bins=num_bins)\n",
        "\n",
        "print(\"Bin Edges: \", outcome_bins)\n",
        "\n",
        "# Create the Bucketizer transformer with the updated splits\n",
        "bucketizer = Bucketizer(splits=outcome_bins, inputCol=\"medv\", outputCol=\"medv_bin\")\n",
        "\n",
        "# Apply the Bucketizer to the data\n",
        "df = bucketizer.transform(df)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShDX3eTyRV7p",
        "outputId": "b55504bc-c309-415d-fe98-5edbcb5faf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin Edges:  [ 6.3  10.55 14.8  19.05 23.3  27.55 31.8  36.05 40.3  44.55 48.8 ]\n",
            "+--------+-----+------+----+------+------+-----+------+---+-----+-------+------+-----+----+--------+\n",
            "|    crim|   zn| indus|chas|   nox|    rm|  age|   dis|rad|  tax|ptratio| black|lstat|medv|medv_bin|\n",
            "+--------+-----+------+----+------+------+-----+------+---+-----+-------+------+-----+----+--------+\n",
            "| 0.00632|18.00| 2.310|   0|0.5380|6.5750|65.20|4.0900|  1|296.0|  15.30|396.90| 4.98|24.0|     4.0|\n",
            "| 0.02731| 0.00| 7.070|   0|0.4690|6.4210|78.90|4.9671|  2|242.0|  17.80|396.90| 9.14|21.6|     3.0|\n",
            "| 0.02729| 0.00| 7.070|   0|0.4690|7.1850|61.10|4.9671|  2|242.0|  17.80|392.83| 4.03|34.7|     6.0|\n",
            "| 0.03237| 0.00| 2.180|   0|0.4580|6.9980|45.80|6.0622|  3|222.0|  18.70|394.63| 2.94|33.4|     6.0|\n",
            "| 0.06905| 0.00| 2.180|   0|0.4580|7.1470|54.20|6.0622|  3|222.0|  18.70|396.90| 5.33|36.2|     7.0|\n",
            "| 0.02985| 0.00| 2.180|   0|0.4580|6.4300|58.70|6.0622|  3|222.0|  18.70|394.12| 5.21|28.7|     5.0|\n",
            "| 0.08829|12.50| 7.870|   0|0.5240|6.0120|66.60|5.5605|  5|311.0|  15.20|395.60|12.43|22.9|     3.0|\n",
            "| 0.14455|12.50| 7.870|   0|0.5240|6.1720|96.10|5.9505|  5|311.0|  15.20|396.90|19.15|27.1|     4.0|\n",
            "| 0.17004|12.50| 7.870|   0|0.5240|6.0040|85.90|6.5921|  5|311.0|  15.20|386.71|17.10|18.9|     2.0|\n",
            "| 0.22489|12.50| 7.870|   0|0.5240|6.3770|94.30|6.3467|  5|311.0|  15.20|392.52|20.45|15.0|     2.0|\n",
            "| 0.11747|12.50| 7.870|   0|0.5240|6.0090|82.90|6.2267|  5|311.0|  15.20|396.90|13.27|18.9|     2.0|\n",
            "| 0.09378|12.50| 7.870|   0|0.5240|5.8890|39.00|5.4509|  5|311.0|  15.20|390.50|15.71|21.7|     3.0|\n",
            "| 0.62976| 0.00| 8.140|   0|0.5380|5.9490|61.80|4.7075|  4|307.0|  21.00|396.90| 8.26|20.4|     3.0|\n",
            "| 0.63796| 0.00| 8.140|   0|0.5380|6.0960|84.50|4.4619|  4|307.0|  21.00|380.02|10.26|18.2|     2.0|\n",
            "| 0.62739| 0.00| 8.140|   0|0.5380|5.8340|56.50|4.4986|  4|307.0|  21.00|395.62| 8.47|19.9|     3.0|\n",
            "| 1.05393| 0.00| 8.140|   0|0.5380|5.9350|29.30|4.4986|  4|307.0|  21.00|386.85| 6.58|23.1|     3.0|\n",
            "| 0.78420| 0.00| 8.140|   0|0.5380|5.9900|81.70|4.2579|  4|307.0|  21.00|386.75|14.67|17.5|     2.0|\n",
            "| 0.80271| 0.00| 8.140|   0|0.5380|5.4560|36.60|3.7965|  4|307.0|  21.00|288.99|11.69|20.2|     3.0|\n",
            "| 0.72580| 0.00| 8.140|   0|0.5380|5.7270|69.50|3.7965|  4|307.0|  21.00|390.95|11.28|18.2|     2.0|\n",
            "| 1.25179| 0.00| 8.140|   0|0.5380|5.5700|98.10|3.7979|  4|307.0|  21.00|376.57|21.02|13.6|     1.0|\n",
            "+--------+-----+------+----+------+------+-----+------+---+-----+-------+------+-----+----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the result\n",
        "df.select(col(\"medv\"), col(\"medv_bin\")).show()\n",
        "\n",
        "outcomes = df.select(col(\"medv_bin\"))\n",
        "\n",
        "outcomes.distinct().show()\n",
        "\n",
        "outcomes.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18ep9RSyZ1RG",
        "outputId": "33f177e6-79b8-4e3e-c269-9a1852d986e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+\n",
            "|medv|medv_bin|\n",
            "+----+--------+\n",
            "|24.0|     4.0|\n",
            "|21.6|     3.0|\n",
            "|34.7|     6.0|\n",
            "|33.4|     6.0|\n",
            "|36.2|     7.0|\n",
            "|28.7|     5.0|\n",
            "|22.9|     3.0|\n",
            "|27.1|     4.0|\n",
            "|18.9|     2.0|\n",
            "|15.0|     2.0|\n",
            "|18.9|     2.0|\n",
            "|21.7|     3.0|\n",
            "|20.4|     3.0|\n",
            "|18.2|     2.0|\n",
            "|19.9|     3.0|\n",
            "|23.1|     3.0|\n",
            "|17.5|     2.0|\n",
            "|20.2|     3.0|\n",
            "|18.2|     2.0|\n",
            "|13.6|     1.0|\n",
            "+----+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------+\n",
            "|medv_bin|\n",
            "+--------+\n",
            "|     8.0|\n",
            "|     0.0|\n",
            "|     7.0|\n",
            "|     1.0|\n",
            "|     4.0|\n",
            "|     3.0|\n",
            "|     2.0|\n",
            "|     6.0|\n",
            "|     5.0|\n",
            "|     9.0|\n",
            "+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "450"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discrete_df = spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \"\\t\").load(\"housing_discrete.csv\")\n",
        "print(discrete_df.count())\n",
        "discrete_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WISvKBbzRWh8",
        "outputId": "c416a61a-ba66-4d84-ce34-799836f99db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "490\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+\n",
            "|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+\n",
            "|   0|  1|    0|   0|  2|  2|  1|  2|  0|  1|      0|    3|    0|\n",
            "|   0|  0|    1|   0|  1|  2|  2|  2|  0|  0|      1|    3|    1|\n",
            "|   0|  0|    1|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    2|    0|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    3|    0|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# example_slice_filter = (col(\"indus\") == 0) & (col(\"dis\") == 2)\n",
        "# example_slice_df = discrete_df.filter(example_slice_filter)\n",
        "discrete_df.show()\n",
        "\n",
        "example_slice_df = discrete_df.withColumn(\"slice\", \n",
        "                   when((col(\"indus\") == 0) & (col(\"dis\") == 2), True)\n",
        "                   .otherwise(False))\n",
        "\n",
        "print(example_slice_df.count(), \"examples in slice\")\n",
        "example_slice_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHv5hBikRqMy",
        "outputId": "00d1e207-7e2e-4d39-c1b2-c117b3e53fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+\n",
            "|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+\n",
            "|   0|  1|    0|   0|  2|  2|  1|  2|  0|  1|      0|    3|    0|\n",
            "|   0|  0|    1|   0|  1|  2|  2|  2|  0|  0|      1|    3|    1|\n",
            "|   0|  0|    1|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    2|    0|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    3|    0|\n",
            "|   0|  0|    0|   0|  1|  2|  1|  3|  0|  0|      1|    2|    0|\n",
            "|   0|  1|    1|   0|  1|  1|  1|  3|  2|  1|      0|    2|    2|\n",
            "|   0|  1|    1|   0|  1|  1|  3|  3|  2|  1|      0|    3|    3|\n",
            "|   0|  1|    1|   0|  1|  0|  3|  3|  2|  1|      0|    1|    3|\n",
            "|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    1|    2|\n",
            "|   0|  1|    1|   0|  1|  2|  3|  3|  2|  1|      0|    2|    3|\n",
            "|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    3|    2|\n",
            "|   0|  1|    1|   0|  1|  1|  0|  3|  2|  1|      0|    1|    2|\n",
            "|   0|  0|    1|   0|  2|  1|  1|  2|  1|  1|      3|    3|    1|\n",
            "|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    1|\n",
            "|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    2|    1|\n",
            "|   0|  0|    1|   0|  2|  1|  0|  2|  1|  1|      3|    1|    0|\n",
            "|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    2|\n",
            "|   0|  0|    1|   0|  2|  0|  0|  2|  1|  1|      3|    0|    2|\n",
            "|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    1|    1|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "490 examples in slice\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+-----+\n",
            "|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat|slice|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+-----+\n",
            "|   0|  1|    0|   0|  2|  2|  1|  2|  0|  1|      0|    3|    0| true|\n",
            "|   0|  0|    1|   0|  1|  2|  2|  2|  0|  0|      1|    3|    1|false|\n",
            "|   0|  0|    1|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|false|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    2|    0|false|\n",
            "|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    3|    0|false|\n",
            "|   0|  0|    0|   0|  1|  2|  1|  3|  0|  0|      1|    2|    0|false|\n",
            "|   0|  1|    1|   0|  1|  1|  1|  3|  2|  1|      0|    2|    2|false|\n",
            "|   0|  1|    1|   0|  1|  1|  3|  3|  2|  1|      0|    3|    3|false|\n",
            "|   0|  1|    1|   0|  1|  0|  3|  3|  2|  1|      0|    1|    3|false|\n",
            "|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    1|    2|false|\n",
            "|   0|  1|    1|   0|  1|  2|  3|  3|  2|  1|      0|    2|    3|false|\n",
            "|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    3|    2|false|\n",
            "|   0|  1|    1|   0|  1|  1|  0|  3|  2|  1|      0|    1|    2|false|\n",
            "|   0|  0|    1|   0|  2|  1|  1|  2|  1|  1|      3|    3|    1|false|\n",
            "|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    1|false|\n",
            "|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    2|    1|false|\n",
            "|   0|  0|    1|   0|  2|  1|  0|  2|  1|  1|      3|    1|    0|false|\n",
            "|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    2|false|\n",
            "|   0|  0|    1|   0|  2|  0|  0|  2|  1|  1|      3|    0|    2|false|\n",
            "|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    1|    1|false|\n",
            "+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, sum, col, log2\n",
        "\n",
        "outcomes.printSchema()\n",
        "\n",
        "grouped_data = df.groupBy(\"medv_bin\").count()\n",
        "grouped_data = grouped_data.withColumn(\"count\", col(\"count\").cast(\"int\"))\n",
        "grouped_data.show()\n",
        "total_count = grouped_data.agg(sum(\"count\")).collect()[0][0]\n",
        "\n",
        "probabilities = grouped_data.withColumn(\"probability\", ((col(\"count\") / total_count) * log2(col(\"count\") / total_count)))\n",
        "probabilities.show()\n",
        "\n",
        "entropy = probabilities.agg(sum(\"probability\")).collect()[0][0]\n",
        "\n",
        "# filter out null values before taking the logarithm\n",
        "\n",
        "grouped_data.printSchema()\n",
        "grouped_data.count()\n",
        "total_count\n",
        "\n",
        "print (\"entropy: \", entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqsgg1mJSALZ",
        "outputId": "81d5a673-e81f-4109-d5b1-ef36dd48a43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- medv_bin: double (nullable = true)\n",
            "\n",
            "+--------+-----+\n",
            "|medv_bin|count|\n",
            "+--------+-----+\n",
            "|     8.0|    8|\n",
            "|     0.0|   17|\n",
            "|     7.0|   12|\n",
            "|     1.0|   39|\n",
            "|     4.0|   73|\n",
            "|     3.0|  152|\n",
            "|     2.0|   83|\n",
            "|     6.0|   26|\n",
            "|     5.0|   33|\n",
            "|     9.0|    7|\n",
            "+--------+-----+\n",
            "\n",
            "+--------+-----+--------------------+\n",
            "|medv_bin|count|         probability|\n",
            "+--------+-----+--------------------+\n",
            "|     8.0|    8|-0.10335611006608067|\n",
            "|     0.0|   17|-0.17854980433207523|\n",
            "|     7.0|   12|-0.13943516507989018|\n",
            "|     1.0|   39| -0.3057928442707484|\n",
            "|     4.0|   73|-0.42566407591244987|\n",
            "|     3.0|  152| -0.5289105756034771|\n",
            "|     2.0|   83| -0.4498123690427096|\n",
            "|     6.0|   26|-0.23765972955549902|\n",
            "|     5.0|   33|-0.27642171860296283|\n",
            "|     9.0|    7|-0.09343329752025784|\n",
            "+--------+-----+--------------------+\n",
            "\n",
            "root\n",
            " |-- medv_bin: double (nullable = true)\n",
            " |-- count: integer (nullable = false)\n",
            "\n",
            "entropy:  -2.7390356899861508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outcomes_df = spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \"\\t\").load(\"outcome_bins.csv\")\n",
        "outcomes_df.printSchema()\n",
        "outcomes_df = outcomes_df.toDF(\"medv_bin\")\n",
        "outcomes_df.printSchema()\n",
        "\n",
        "grouped_data = outcomes_df.groupBy(\"medv_bin\").count()\n",
        "grouped_data = grouped_data.withColumn(\"count\", col(\"count\").cast(\"int\"))\n",
        "grouped_data.show()\n",
        "total_count = grouped_data.agg(sum(\"count\")).collect()[0][0]\n",
        "\n",
        "probabilities = grouped_data.withColumn(\"probability\", ((col(\"count\") / total_count) * log2(col(\"count\") / total_count)))\n",
        "probabilities.show()\n",
        "\n",
        "entropy = probabilities.agg(sum(\"probability\")).collect()[0][0]\n",
        "\n",
        "# filter out null values before taking the logarithm\n",
        "\n",
        "grouped_data.printSchema()\n",
        "grouped_data.count()\n",
        "total_count\n",
        "\n",
        "print (\"entropy: \", entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppyif_SyoeYy",
        "outputId": "8798a602-29ac-4c37-ad2f-e94b236a09ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- 0: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- medv_bin: string (nullable = true)\n",
            "\n",
            "+--------+-----+\n",
            "|medv_bin|count|\n",
            "+--------+-----+\n",
            "|       7|   30|\n",
            "|      11|    1|\n",
            "|       3|   81|\n",
            "|       8|   13|\n",
            "|       5|   96|\n",
            "|       6|   39|\n",
            "|       9|    8|\n",
            "|       1|   21|\n",
            "|      10|    6|\n",
            "|       4|  147|\n",
            "|       2|   48|\n",
            "+--------+-----+\n",
            "\n",
            "+--------+-----+--------------------+\n",
            "|medv_bin|count|         probability|\n",
            "+--------+-----+--------------------+\n",
            "|       7|   30| -0.2467192251057583|\n",
            "|      11|    1|-0.01823803661020933|\n",
            "|       3|   81| -0.4292649445419462|\n",
            "|       8|   13|-0.13891954463510045|\n",
            "|       5|   96| -0.4607364123979914|\n",
            "|       6|   39|-0.29060855731729096|\n",
            "|       9|    8|-0.09692470104493994|\n",
            "|       1|   21|  -0.194756593552449|\n",
            "|      10|    6|-0.07777561761160916|\n",
            "|       4|  147| -0.5210896782498619|\n",
            "|       2|   48|-0.32832738987246507|\n",
            "+--------+-----+--------------------+\n",
            "\n",
            "root\n",
            " |-- medv_bin: string (nullable = true)\n",
            " |-- count: integer (nullable = false)\n",
            "\n",
            "entropy:  -2.8033607009396215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id \n",
        "\n",
        "example_slice_df = example_slice_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "outcomes_df = outcomes_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "merged_df = example_slice_df.join(outcomes_df, \"id\")\n",
        "\n",
        "merged_df.show()\n",
        "\n",
        "example_slice_df = merged_df.filter((col(\"slice\") == \"true\")).select(\"medv_bin\")\n",
        "\n",
        "example_slice_df.count()\n",
        "example_slice_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTFGjjlkCoMC",
        "outputId": "e07cab7b-6a9e-4fe7-f709-9dcbc52b0a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+-----+--------+\n",
            "| id|crim| zn|indus|chas|nox| rm|age|dis|rad|tax|ptratio|black|lstat|slice|medv_bin|\n",
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+-----+--------+\n",
            "|  0|   0|  1|    0|   0|  2|  2|  1|  2|  0|  1|      0|    3|    0| true|       5|\n",
            "|  1|   0|  0|    1|   0|  1|  2|  2|  2|  0|  0|      1|    3|    1|false|       4|\n",
            "|  2|   0|  0|    1|   0|  1|  3|  1|  2|  0|  0|      1|    2|    0|false|       7|\n",
            "|  3|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    2|    0|false|       7|\n",
            "|  4|   0|  0|    0|   0|  1|  3|  1|  3|  0|  0|      1|    3|    0|false|       8|\n",
            "|  5|   0|  0|    0|   0|  1|  2|  1|  3|  0|  0|      1|    2|    0|false|       6|\n",
            "|  6|   0|  1|    1|   0|  1|  1|  1|  3|  2|  1|      0|    2|    2|false|       5|\n",
            "|  7|   0|  1|    1|   0|  1|  1|  3|  3|  2|  1|      0|    3|    3|false|       6|\n",
            "|  8|   0|  1|    1|   0|  1|  0|  3|  3|  2|  1|      0|    1|    3|false|       3|\n",
            "|  9|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    1|    2|false|       4|\n",
            "| 10|   0|  1|    1|   0|  1|  2|  3|  3|  2|  1|      0|    2|    3|false|       3|\n",
            "| 11|   0|  1|    1|   0|  1|  1|  2|  3|  2|  1|      0|    3|    2|false|       4|\n",
            "| 12|   0|  1|    1|   0|  1|  1|  0|  3|  2|  1|      0|    1|    2|false|       4|\n",
            "| 13|   0|  0|    1|   0|  2|  1|  1|  2|  1|  1|      3|    3|    1|false|       4|\n",
            "| 14|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    1|false|       4|\n",
            "| 15|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    2|    1|false|       4|\n",
            "| 16|   0|  0|    1|   0|  2|  1|  0|  2|  1|  1|      3|    1|    0|false|       5|\n",
            "| 17|   0|  0|    1|   0|  2|  1|  2|  2|  1|  1|      3|    1|    2|false|       3|\n",
            "| 18|   0|  0|    1|   0|  2|  0|  0|  2|  1|  1|      3|    0|    2|false|       4|\n",
            "| 19|   0|  0|    1|   0|  2|  0|  1|  2|  1|  1|      3|    1|    1|false|       4|\n",
            "+---+----+---+-----+----+---+---+---+---+---+---+-------+-----+-----+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------+\n",
            "|medv_bin|\n",
            "+--------+\n",
            "|       5|\n",
            "|       4|\n",
            "|       7|\n",
            "|       7|\n",
            "|       8|\n",
            "|       6|\n",
            "|       5|\n",
            "|       6|\n",
            "|       3|\n",
            "|       4|\n",
            "|       3|\n",
            "|       4|\n",
            "|       4|\n",
            "|       4|\n",
            "|       4|\n",
            "|       4|\n",
            "|       5|\n",
            "|       3|\n",
            "|       4|\n",
            "|       4|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_data = example_slice_df.groupBy(\"medv_bin\").count()\n",
        "grouped_data = grouped_data.withColumn(\"count\", col(\"count\").cast(\"int\"))\n",
        "grouped_data.show()\n",
        "total_count = grouped_data.agg(sum(\"count\")).collect()[0][0]\n",
        "\n",
        "probabilities = grouped_data.withColumn(\"probability\", ((col(\"count\") / total_count) * log2(col(\"count\") / total_count)))\n",
        "probabilities.show()\n",
        "\n",
        "slice_entropy = probabilities.agg(sum(\"probability\")).collect()[0][0]\n",
        "\n",
        "# filter out null values before taking the logarithm\n",
        "\n",
        "grouped_data.printSchema()\n",
        "grouped_data.count()\n",
        "total_count\n",
        "\n",
        "print (\"entropy: \", entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii9W5R3JMdaG",
        "outputId": "31c266c8-eca9-461e-a46f-649601028548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|medv_bin|count|\n",
            "+--------+-----+\n",
            "|       7|    2|\n",
            "|       3|    8|\n",
            "|       8|    1|\n",
            "|       5|    3|\n",
            "|       6|    2|\n",
            "|       4|   10|\n",
            "|       2|    1|\n",
            "+--------+-----+\n",
            "\n",
            "+--------+-----+--------------------+\n",
            "|medv_bin|count|         probability|\n",
            "+--------+-----+--------------------+\n",
            "|       7|    2|-0.27813981497507173|\n",
            "|       3|    8| -0.5199666673076944|\n",
            "|       8|    1|-0.17610694452457293|\n",
            "|       5|    3| -0.3522138890491458|\n",
            "|       6|    2|-0.27813981497507173|\n",
            "|       4|   10| -0.5307257063985579|\n",
            "|       2|    1|-0.17610694452457293|\n",
            "+--------+-----+--------------------+\n",
            "\n",
            "root\n",
            " |-- medv_bin: string (nullable = true)\n",
            " |-- count: integer (nullable = false)\n",
            "\n",
            "entropy:  -2.3113997817546874\n"
          ]
        }
      ]
    }
  ]
}